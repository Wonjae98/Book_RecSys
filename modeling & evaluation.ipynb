{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e20ca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408b13a2",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf721b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeDuplicateData():\n",
    "    info_original = pd.read_csv('BX-Books.csv',sep=';', on_bad_lines='skip', encoding='latin-1', low_memory=False).sort_values('ISBN')\n",
    "    info_original.drop(['Book-Author','Year-Of-Publication','Publisher','Image-URL-S','Image-URL-M','Image-URL-L'],axis=1 ,inplace=True)\n",
    "\n",
    "    ratings = pd.read_csv('BX-Book-Ratings.csv',sep=';', encoding='latin-1')\n",
    "    users = pd.read_csv('BX-Users.csv', sep=';', encoding='latin-1')\n",
    "\n",
    "    ratings = ratings[ratings['ISBN'].isin(info_original['ISBN'])]\n",
    "    ratings = ratings[ratings['User-ID'].isin(users['User-ID'])].reset_index(drop=True)\n",
    "\n",
    "    info_name_modified = info_original.copy()\n",
    "    info_name_modified['Book-Title'] = info_name_modified['Book-Title'].str.replace(pat=r'[^\\w]',repl=r'',regex=True)\n",
    "    info_name_modified['Book-Title'] = info_name_modified['Book-Title'].str.upper()\n",
    "\n",
    "    info_no_duplicate = info_name_modified.drop_duplicates(['Book-Title'],keep='first')\n",
    "    \n",
    "    for index, ISBN in enumerate(ratings['ISBN']):\n",
    "        if(ISBN not in list(info_no_duplicate['ISBN'])):\n",
    "            index_to_change = info_name_modified[info_name_modified['ISBN'] == ISBN].reset_index(drop=True)\n",
    "            book_title = index_to_change.at[0,'Book-Title']\n",
    "            new_ISBN = info_name_modified[info_name_modified['Book-Title'] == book_title].reset_index(drop=True)\n",
    "            new_ISBN = new_ISBN.at[0,'ISBN']\n",
    "            ratings.loc[ratings['ISBN'] == ISBN,'ISBN'] = new_ISBN\n",
    "            print(index, book_title,ISBN, 'to',new_ISBN)\n",
    "\n",
    "    for index, ISBN in enumerate(info_no_duplicate['ISBN']):\n",
    "        index_to_change = info_original[info_original['ISBN'] == ISBN].reset_index(drop=True)\n",
    "        book_title = index_to_change.at[0,'Book-Title']\n",
    "        info_no_duplicate.loc[info_no_duplicate['ISBN'] == ISBN,'Book-Title'] = book_title\n",
    "        print(index, ISBN, book_title)\n",
    "        \n",
    "    info_no_duplicate.to_csv(\"modified_book.csv\",index=False)\n",
    "    ratings.to_csv(\"modified_rating.csv\",index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33b6b3f8",
   "metadata": {},
   "source": [
    "Erases inconsistent data and integrates ISBNs from the same books. It takes quite a long time, so I'll save it and bring it back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3643bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('modified_rating.csv', encoding='latin-1')\n",
    "info = pd.read_csv('modified_book.csv', encoding='latin-1')\n",
    "\n",
    "ratings['ISBN'] = ratings['ISBN'].apply(lambda x : x.zfill(10))\n",
    "\n",
    "ratings_group = ratings.groupby('User-ID').count()\n",
    "ratings_group = ratings_group[ratings_group['ISBN'] >= 10]\n",
    "ratings = ratings.loc[ratings['User-ID'].isin(ratings_group.index)]\n",
    "\n",
    "books_group = ratings.groupby('ISBN').count()\n",
    "books_group = books_group[books_group['User-ID'] >= 10]\n",
    "ratings = ratings.loc[ratings['ISBN'].isin(books_group.index)]\n",
    "\n",
    "ratings = ratings.loc[ratings['Book-Rating']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a84f74",
   "metadata": {},
   "source": [
    "Consolidate ISBN to 10 digits and erase books and people with less than 10 evaluations from the data and filter out zero-point data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1ca9fd",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f43995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_sample = 100\n",
    "\n",
    "num_of_components = 15\n",
    "\n",
    "num_of_recommend = 20\n",
    "\n",
    "ISBN_to_recommend = '0451118642'\n",
    "\n",
    "user_history = [[0,'0451118642',9],\n",
    "                [0,'0345243447',10],\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3a054f",
   "metadata": {},
   "source": [
    "Declare Required Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba797582",
   "metadata": {},
   "source": [
    "## contents-based filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80269dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contentBasedRecommend(df, books_group, ISBN_to_recommend , num_of_recommend):\n",
    "    meta = pd.read_csv('Preprocessed_data.csv',on_bad_lines='skip', low_memory=False)\n",
    "    meta.drop(['Unnamed: 0','location','user_id','rating','book_title','year_of_publication','publisher','img_s','img_m','img_l','Language','city','state','country','age'],axis=1 ,inplace=True)\n",
    "    meta.rename(columns = {'isbn':'ISBN'},inplace=True)\n",
    "    meta = meta.sort_values('ISBN').drop_duplicates(['ISBN'],keep='first')\n",
    "\n",
    "    df = pd.merge(df, meta, on = 'ISBN')\n",
    "    df = df.loc[(df['Summary'] != '9') & (df['Category'] != '9'),:]\n",
    "    df = df.loc[df['ISBN'].isin(books_group.index)].reset_index(drop=True)\n",
    "\n",
    "    df['index'] = df.index.values\n",
    "    book = df[df['ISBN'] == ISBN_to_recommend]\n",
    "\n",
    "    to_join = ['Book-Title','book_author','Summary','Category']\n",
    "    df['features'] = [' '.join(df[to_join].iloc[i,].values) for i in range(df[to_join].shape[0])]\n",
    "\n",
    "    tf = TfidfVectorizer(stop_words='english',min_df=1,ngram_range=(1,5))\n",
    "    matrix = tf.fit_transform(df['features'])\n",
    "    cos= cosine_similarity(matrix)\n",
    "\n",
    "    recommendation_list = list(enumerate(cos[book.index.values[0]]))\n",
    "    recommendation_list = sorted(recommendation_list,key=lambda x:x[1],reverse=True)[1:num_of_recommend+1]\n",
    "\n",
    "    books =[]\n",
    "    for i in range(len(recommendation_list)):\n",
    "        books.append([df[df['index'] == recommendation_list[i][0]]['Book-Title'].item(),recommendation_list[i][1]])\n",
    "\n",
    "    print(pd.DataFrame(books).set_index(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df73e25",
   "metadata": {},
   "source": [
    "Get metadata and combine it with data, \n",
    "combine 'Book-Title', 'book_author', 'Summary', and 'Category' into one feature to calculate the TF-IDF matrix\n",
    "and recommend a book based on cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd8bd4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                           1\n",
      "0                                                           \n",
      "2010: Odyssey Two                                   0.048847\n",
      "Rama Revealed (Bantam Spectra Book)                 0.028845\n",
      "Bloodletter (Star Trek Deep Space Nine, No 3)       0.023130\n",
      "The Odyssey (Penguin Classics)                      0.021169\n",
      "Dark Tide II: Ruin (Star Wars: The New Jedi Ord...  0.021156\n",
      "The Ship Who Sang                                   0.020857\n",
      "Star Wars: The Han Solo Adventures/3 Books in One   0.019922\n",
      "Pegasus in Space                                    0.018672\n",
      "The Andromeda Strain                                0.018574\n",
      "The Light of Other Days                             0.018360\n",
      "Dawn (Cutler)                                       0.017486\n",
      "Rama II: The Sequel to Rendezvous with Rama         0.017293\n",
      "Love in the Ruins                                   0.016784\n",
      "Refugee (Bio of a Space Tyrant, Vol 1)              0.016722\n",
      "LIFE UNIVERS EVRTH (Hitchhiker's Trilogy (Paper...  0.016618\n",
      "Smack                                               0.016403\n",
      "About That Man                                      0.016285\n",
      "I Dreamed of Africa                                 0.016144\n",
      "The Siege (Star Trek Deep Space Nine, No 2)         0.016049\n",
      "The Return                                          0.015986\n"
     ]
    }
   ],
   "source": [
    "contentBasedRecommend(info, books_group, ISBN_to_recommend, num_of_recommend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0031140",
   "metadata": {},
   "source": [
    "## item-based collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d6883d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(ratings, info, on = 'ISBN').sort_values('User-ID')\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "df['User-ID'] = encoder.fit_transform(df['User-ID'])\n",
    "\n",
    "pivot_table = df.pivot_table(values='Book-Rating',index='User-ID', columns='ISBN').dropna(how = 'all').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a651905",
   "metadata": {},
   "source": [
    "data merge and make pivot table to SVD calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eb9913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def itemBasedRecommend(pivot_table, df,ISBN_to_recommend , num_of_components, num_of_recommend):\n",
    "    scaler = MinMaxScaler()\n",
    "    item_sim = cosine_similarity(pivot_table.T, pivot_table.T)\n",
    "\n",
    "    SVD = TruncatedSVD(n_components = num_of_components, algorithm = 'arpack')\n",
    "    matrix = SVD.fit_transform(pivot_table.T)\n",
    "\n",
    "    item_sim_df = pd.DataFrame(data=item_sim,index=pivot_table.columns,columns=pivot_table.columns).drop([ISBN_to_recommend],axis = 0)\n",
    "    corr= pd.DataFrame(np.corrcoef(matrix), columns = pivot_table.columns,index = pivot_table.columns).drop([ISBN_to_recommend],axis = 0)\n",
    "\n",
    "    pivot_table = pivot_table.drop([ISBN_to_recommend],axis = 1)\n",
    "\n",
    "    cos = pd.DataFrame(scaler.fit_transform(item_sim_df[ISBN_to_recommend][:, np.newaxis]),index=pivot_table.columns,columns=[ISBN_to_recommend])\n",
    "    corr = pd.DataFrame(scaler.fit_transform(corr[ISBN_to_recommend][:, np.newaxis]),index=pivot_table.columns,columns=[ISBN_to_recommend])\n",
    "    \n",
    "    recommendation_list = cos*0.75 + corr*0.25\n",
    "    recommendation_list = recommendation_list.sort_values(by=ISBN_to_recommend, ascending=False)[:num_of_recommend]\n",
    "    \n",
    "    book_title_list = []\n",
    "    for i in recommendation_list.index :\n",
    "        index_to_change = df[df['ISBN'] == i].reset_index(drop=True)\n",
    "        book_title_list.append(index_to_change.at[0,'Book-Title'])\n",
    "\n",
    "    recommendation_list.index = book_title_list\n",
    "    \n",
    "    print(recommendation_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94308f62",
   "metadata": {},
   "source": [
    "After reducing the dimension of the pivot table through SVD, calculate the cosine similarity and correlation, respectively, multiply the weight, and recommend the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fbcb303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kim\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:13: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  del sys.path[0]\n",
      "C:\\Users\\Kim\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:14: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    0451118642\n",
      "The Death and Life of Superman: A Novel               0.838511\n",
      "2010: Odyssey Two                                     0.818614\n",
      "The Rabbi                                             0.731573\n",
      "The Ultimate Hitchhiker's Guide to the Galaxy         0.726015\n",
      "Heart Of The Mat (Harlequin Romance, No 2876)         0.721143\n",
      "36 Hours Christmas (Silhouette Promo)                 0.721143\n",
      "Gossip Girl                                           0.721143\n",
      "The Beach Club : A Novel                              0.721143\n",
      "Informed Consent                                      0.721143\n",
      "Combat, Vol. 1 (Combat)                               0.721143\n",
      "Books and Reading: A Book of Quotations (Dover ...    0.721143\n",
      "Das WÃ?ÃÂ¼ten der ganzen Welt.                      0.721143\n",
      "Panama: A Novel                                       0.721143\n",
      "The Sands of Sakkara                                  0.721143\n",
      "Rainsong                                              0.721143\n",
      "The Yellow Room                                       0.721143\n",
      "A Valentine Wedding                                   0.721143\n",
      "Death Rounds                                          0.721143\n",
      "Cosi fan tutte. Eine Geschichte.                      0.721143\n",
      "Cooks Overboard (Angie Amalfi Mysteries)              0.721143\n"
     ]
    }
   ],
   "source": [
    "itemBasedRecommend(pivot_table, df, ISBN_to_recommend, num_of_components, num_of_recommend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328d7441",
   "metadata": {},
   "source": [
    "## user-based collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bc3b4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeNewUserData(user_history, rating_data):\n",
    "    user_data = pd.DataFrame(user_history,columns=['User-ID','ISBN','Book-Rating'])\n",
    "\n",
    "    return pd.concat([user_data,rating_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae5ea101",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_history = [[0,'0451118642',9],\n",
    "                [0,'0345243447',10],\n",
    "                ]\n",
    "\n",
    "ratings_include_history = makeNewUserData(user_history, ratings)\n",
    "\n",
    "df = pd.merge(ratings_include_history, info, on = 'ISBN').sort_values('User-ID')\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "df['User-ID'] = encoder.fit_transform(df['User-ID'])\n",
    "\n",
    "pivot_table = df.pivot_table(values='Book-Rating',index='User-ID', columns='ISBN').dropna(how = 'all').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75459a1c",
   "metadata": {},
   "source": [
    "After adding the user's rating data, data merge and make pivot table to SVD calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a15ddfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def userBasedRecommend(pivot_table, df, num_of_components, num_of_recommend, want_print = True):\n",
    "    ratings = pivot_table.values\n",
    "    ratings_mean = np.mean(ratings,axis = 1)\n",
    "    ratings = ratings - ratings_mean.reshape(-1,1)\n",
    "\n",
    "    SVD = TruncatedSVD(n_components = num_of_components, algorithm = 'arpack')\n",
    "    U = SVD.fit_transform(ratings)\n",
    "    sigma=SVD.explained_variance_ratio_\n",
    "    Vt= SVD.components_\n",
    "\n",
    "    matrix_to_predict = np.dot(np.dot(U, np.diag(sigma)), Vt) + ratings_mean.reshape(-1,1)\n",
    "    df_to_preditct = pd.DataFrame(matrix_to_predict, columns = pivot_table.columns,index = pivot_table.index)\n",
    "\n",
    "    user_data = df[df['User-ID'] == 0]\n",
    "\n",
    "    user_prediction_list = df_to_preditct.iloc[0].sort_values(ascending=False)\n",
    "    recommendation_list = user_prediction_list[~user_prediction_list.index.isin(user_data['ISBN'])][:num_of_recommend]\n",
    "\n",
    "    recomendation_ISBN = recommendation_list.copy()\n",
    "\n",
    "    if(want_print):\n",
    "        book_title_list = []\n",
    "        for i in recommendation_list.index :\n",
    "            index_to_change = df[df['ISBN'] == i].reset_index(drop=True)\n",
    "            book_title_list.append(index_to_change.at[0,'Book-Title'])\n",
    "\n",
    "        recommendation_list.index = book_title_list\n",
    "        print('\\n',recommendation_list)\n",
    "    \n",
    "    return recomendation_ISBN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38700d29",
   "metadata": {},
   "source": [
    "Decompose the matrix and restore it to recommend a book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7255c51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The Two Towers (The Lord of the Rings, Part 2)                0.001577\n",
      "The Fellowship of the Ring (The Lord of the Rings, Part 1)    0.001570\n",
      "Interview With the Vampire                                    0.001561\n",
      "The Return of the King (The Lord of the Rings, Part 3)        0.001561\n",
      "The Catcher in the Rye                                        0.001522\n",
      "The Golden Compass (His Dark Materials, Book 1)               0.001515\n",
      "The Vampire Lestat (Vampire Chronicles, Book II)              0.001510\n",
      "Ender's Game (Ender Wiggins Saga (Paperback))                 0.001506\n",
      "Life of Pi                                                    0.001500\n",
      "The Handmaid's Tale                                           0.001487\n",
      "Red Dragon                                                    0.001486\n",
      "Brave New World                                               0.001483\n",
      "A Prayer for Owen Meany                                       0.001483\n",
      "The Hitchhiker's Guide to the Galaxy                          0.001465\n",
      "Silence of the Lambs                                          0.001463\n",
      "American Gods                                                 0.001463\n",
      "1984                                                          0.001448\n",
      "To Kill a Mockingbird                                         0.001446\n",
      "Dune (Remembering Tomorrow)                                   0.001438\n",
      "Snow Falling on Cedars                                        0.001437\n",
      "Name: 0, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ISBN\n",
       "0345339711    0.001577\n",
       "0345339703    0.001570\n",
       "0345256085    0.001561\n",
       "0345339738    0.001561\n",
       "0316769177    0.001522\n",
       "0345413350    0.001515\n",
       "0345313860    0.001510\n",
       "0312853238    0.001506\n",
       "0151008116    0.001500\n",
       "0395404258    0.001487\n",
       "0385319673    0.001486\n",
       "0001047973    0.001483\n",
       "0345361792    0.001483\n",
       "0345391802    0.001465\n",
       "0312022824    0.001463\n",
       "0380789035    0.001463\n",
       "0151660387    0.001448\n",
       "006017322X    0.001446\n",
       "0441172717    0.001438\n",
       "0151001006    0.001437\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userBasedRecommend(pivot_table, df, num_of_components, num_of_recommend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635b020c",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09026f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifyRecommendation(ratings, info, num_of_sample, num_of_components, num_of_recommend):\n",
    "    \n",
    "    ratings_group = ratings.groupby('User-ID').count()\n",
    "    ratings_group = ratings_group[ratings_group['ISBN'] >= 100]\n",
    "    total_correct=0\n",
    "\n",
    "    for i in range(num_of_sample):\n",
    "        user_test_id = ratings_group.sample(n=1).index.values[0]\n",
    "\n",
    "        user_test_df=ratings[ratings['User-ID']==user_test_id]\n",
    "        ratings_exclude_test=ratings[ratings['User-ID']!=user_test_id]\n",
    "\n",
    "        user_test_df=user_test_df[user_test_df['Book-Rating']>=7]\n",
    "        user_test_df=user_test_df.sort_values(by='Book-Rating', ascending=False)\n",
    "\n",
    "        user_test=user_test_df.sample(frac=0.4)\n",
    "        user_test['User-ID'] = np.zeros((len(user_test),1))\n",
    "\n",
    "        user_history=user_test.values.tolist()\n",
    "    \n",
    "        ratings_include_history = makeNewUserData(user_history, ratings_exclude_test)\n",
    "\n",
    "        df = pd.merge(ratings_include_history, info, on = 'ISBN').sort_values('User-ID')\n",
    "\n",
    "        encoder = LabelEncoder()\n",
    "        df['User-ID'] = encoder.fit_transform(df['User-ID'])\n",
    "\n",
    "        pivot_table = df.pivot_table(values='Book-Rating',index='User-ID', columns='ISBN').dropna(how = 'all').fillna(0)\n",
    "    \n",
    "        recommendation_list = userBasedRecommend(pivot_table, df, num_of_components, num_of_recommend, False)\n",
    "\n",
    "        print('\\n','sample', i+1) \n",
    "    \n",
    "        correct = []   \n",
    "        for i in recommendation_list.index :\n",
    "            if(i in list(user_test_df['ISBN'])):\n",
    "                correct.append([i,user_test_df[user_test_df['ISBN']==i]['Book-Rating'].values[0]])\n",
    "       \n",
    "        for i in correct :\n",
    "            index_to_change = df[df['ISBN'] == i[0]].reset_index(drop=True)\n",
    "            i[0] = index_to_change.at[0,'Book-Title']\n",
    "            print(i)\n",
    "\n",
    "\n",
    "        total_correct=total_correct+len(correct)\n",
    "        total_num_recommend=num_of_sample*num_of_recommend\n",
    "        \n",
    "        print(len(correct),\"/\" ,num_of_recommend, \"matched\")\n",
    "\n",
    "    percent=total_correct/total_num_recommend*100\n",
    "    \n",
    "    print()\n",
    "\n",
    "    print(\"Total Matched : \", percent ,\"%\",\"(\",total_correct,\"/\",total_num_recommend,\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0491238",
   "metadata": {},
   "source": [
    "We take samples from the data at random, and we put only 40 percent of the data in the sample and we train them And compare the recommended book with the 60 percent left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4ec566f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " sample 1\n",
      "['Harry Potter and the Chamber of Secrets (Book 2)', 10]\n",
      "[\"Harry Potter and the Sorcerer's Stone (Book 1)\", 10]\n",
      "2 / 20 matched\n",
      "\n",
      " sample 2\n",
      "['It', 9]\n",
      "['The Tommyknockers', 7]\n",
      "2 / 20 matched\n",
      "\n",
      " sample 3\n",
      "['The Red Tent (Bestselling Backlist)', 9]\n",
      "['Snow Falling on Cedars', 9]\n",
      "['The Poisonwood Bible: A Novel', 8]\n",
      "3 / 20 matched\n",
      "\n",
      " sample 4\n",
      "['The Secret Life of Bees', 10]\n",
      "[\"Tuesdays with Morrie: An Old Man, a Young Man, and Life's Greatest Lesson\", 9]\n",
      "2 / 20 matched\n",
      "\n",
      " sample 5\n",
      "['The Nanny Diaries: A Novel', 8]\n",
      "['The Hours : A Novel', 10]\n",
      "['House of Sand and Fog', 7]\n",
      "['Wicked : The Life and Times of the Wicked Witch of the West', 7]\n",
      "['Bridget Jones: The Edge of Reason', 8]\n",
      "5 / 20 matched\n",
      "\n",
      "Total Matched :  14.000000000000002 % ( 14 / 100 )\n"
     ]
    }
   ],
   "source": [
    "num_of_sample = 5\n",
    "\n",
    "num_of_recommend = 20\n",
    "\n",
    "verifyRecommendation(ratings, info, num_of_sample, num_of_components, num_of_recommend)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
